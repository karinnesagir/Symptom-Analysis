{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (320, 2), 'validation': (80, 2)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"ktgiahieu/maccrobat2018_2020\")\n",
    "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
    "validation_ds = dataset.pop('test')\n",
    "dataset['validation'] = validation_ds\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokens and tags\n",
    "tokens = [item['tokens'] for item in dataset['train']]\n",
    "tags = [item['tags'] for item in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define states and observations\n",
    "state_set = set(tag for doc in tags for tag in doc)\n",
    "observation_set = set(token for doc in tokens for token in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize probability matrices\n",
    "transition_probabilities = {state: {state2: 0 for state2 in state_set} for state in state_set}\n",
    "emission_probabilities = {state: {observation: 0 for observation in observation_set} for state in state_set}\n",
    "initial_state_probabilities = {state: 0 for state in state_set} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_list, tag_list in zip(tokens, tags):\n",
    "    previous_tag = None\n",
    "    for token, tag in zip(token_list, tag_list):\n",
    "        emission_probabilities[tag][token] += 1\n",
    "        if previous_tag is not None:\n",
    "            transition_probabilities[previous_tag][tag] += 1\n",
    "        else:\n",
    "            initial_state_probabilities[tag] += 1\n",
    "        previous_tag = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_probabilities(transition_probabilities, emission_probabilities, initial_state_probabilities):\n",
    "    # Normalize transition probabilities\n",
    "    for state, transitions in transition_probabilities.items():\n",
    "        total = sum(transitions.values())\n",
    "        if total > 0:\n",
    "            for state2 in transitions:\n",
    "                transition_probabilities[state][state2] /= total\n",
    "\n",
    "    # Normalize emission probabilities\n",
    "    for state, emissions in emission_probabilities.items():\n",
    "        total = sum(emissions.values())\n",
    "        if total > 0:\n",
    "            for observation in emissions:\n",
    "                emission_probabilities[state][observation] /= total\n",
    "\n",
    "    # Normalize initial state probabilities\n",
    "    total = sum(initial_state_probabilities.values())\n",
    "    if total > 0:\n",
    "        for state in initial_state_probabilities:\n",
    "            initial_state_probabilities[state] /= total\n",
    "\n",
    "    return transition_probabilities, emission_probabilities, initial_state_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities, emission_probabilities, initial_state_probabilities = normalize_probabilities(transition_probabilities, emission_probabilities, initial_state_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "82\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "print(len(transition_probabilities))\n",
    "print(len(emission_probabilities))\n",
    "print(len(initial_state_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(observations, states, start_prob, trans_prob, emit_prob):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    for state in states:\n",
    "        V[0][state] = start_prob.get(state, 0) * emit_prob[state].get(observations[0], 0)\n",
    "        path[state] = [state]\n",
    "\n",
    "    for t in range(1, len(observations)):\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "\n",
    "        for current_state in states:\n",
    "            prob, prev_st = max(\n",
    "                (V[t-1][prev_state] * trans_prob[prev_state].get(current_state, 0) * emit_prob[current_state].get(observations[t], 0), prev_state) \n",
    "                for prev_state in states\n",
    "            )\n",
    "\n",
    "            V[t][current_state] = prob\n",
    "            new_path[current_state] = path[prev_st] + [current_state]\n",
    "\n",
    "        path = new_path\n",
    "\n",
    "    n = len(observations) - 1\n",
    "    prob, max_final_state = max((V[n][state], state) for state in states)\n",
    "    return prob, path[max_final_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, states, initial_state_probabilities, transition_probabilities, emission_probabilities):\n",
    "    tokens = [item['tokens'] for item in dataset['validation']]\n",
    "    tags = [item['tags'] for item in dataset['validation']]\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for token_sequence, true_tag_sequence in zip(tokens, tags):\n",
    "        _, predicted_tag_sequence = viterbi_algorithm(token_sequence, states, initial_state_probabilities, transition_probabilities, emission_probabilities)\n",
    "\n",
    "        total += len(true_tag_sequence)\n",
    "        correct += sum(p_tag == t_tag for p_tag, t_tag in zip(predicted_tag_sequence, true_tag_sequence))\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-Age', 'I-Age', 'I-Age', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [\"The\", \"64\", \"year\", \"old\", \"patient\", \"received\", \"medication\", \".\"]\n",
    "\n",
    "_, predicted_tag_sequence = viterbi_algorithm(test, list(state_set), initial_state_probabilities, transition_probabilities, emission_probabilities)\n",
    "\n",
    "predicted_tag_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(dataset, list(state_set), initial_state_probabilities, transition_probabilities, emission_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7850949850476538\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of tag sequences: 2.125497866231477\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_entropy(tags):\n",
    "    # Flatten the list of tags to a single list\n",
    "    all_tags = [tag for seq in tags for tag in seq]\n",
    "    \n",
    "    # Count the frequency of each tag\n",
    "    tag_counts = Counter(all_tags)\n",
    "    \n",
    "    # Total number of tags\n",
    "    total_tags = len(all_tags)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -sum((count/total_tags) * math.log(count/total_tags, 2) for count in tag_counts.values())\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Example usage\n",
    "tags = [item['tags'] for item in dataset['train']]\n",
    "entropy = calculate_entropy(tags)\n",
    "print(\"Entropy of tag sequences:\", entropy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
